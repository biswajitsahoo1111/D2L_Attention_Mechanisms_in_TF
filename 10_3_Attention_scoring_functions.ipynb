{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow Version:  2.4.1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Tensorflow Version: \", tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from saved_func_show_heatmaps import show_heatmaps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Masked softmax operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequence_mask(X, valid_len, value = 0):\n",
    "    \"\"\"Mask irrelevant entries in sequences.\"\"\"\n",
    "    maxlen = X.shape[1]\n",
    "    mask = (tf.range(start = 0, limit = maxlen, dtype = tf.float32)[None, :] < tf.cast(valid_len[:, None],\n",
    "                                                                                       dtype = tf.float32)).numpy()\n",
    "    X = X.numpy()\n",
    "    X[~mask] = value\n",
    "    return tf.constant(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_softmax(X, valid_lens):\n",
    "    \"\"\"X: 3D tensor\n",
    "       valid_lens: 1D or 2D tensor\n",
    "       When valid_lens is 1D, its `len(valid_lens)` should be same as `X.shape[0]`.\n",
    "       When valid_lens is 2D, `valid_lens.shape` should be same as `X.shape[:2]`.\n",
    "    \"\"\"\n",
    "    if valid_lens is None:\n",
    "        return tf.nn.softmax(X, axis = -1)\n",
    "    else:\n",
    "        shape = X.shape\n",
    "        if len(valid_lens.shape) == 1:\n",
    "            if (len(valid_lens) == X.shape[0]): \n",
    "                valid_lens = tf.repeat(valid_lens, repeats = shape[1])\n",
    "            else:\n",
    "                print(\"Valid_lens shape is incompatible with input. Read docstring of `masked_softmax` function.\")\n",
    "        else:\n",
    "            if (valid_lens.shape == X.shape[:2]):\n",
    "                valid_lens = tf.reshape(valid_lens, shape = -1)\n",
    "            else:\n",
    "                print(\"Valid_lens shape is incompatible with input. Read docstring of `masked_softmax` function.\")\n",
    "            \n",
    "        \n",
    "        X = sequence_mask(tf.reshape(X, shape = (-1, shape[-1])), valid_lens, value=-1e6)    \n",
    "        return tf.nn.softmax(tf.reshape(X, shape = shape), axis = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 2, 4), dtype=float32, numpy=\n",
       "array([[[0.3916409 , 0.6083591 , 0.        , 0.        ],\n",
       "        [0.50549453, 0.49450547, 0.        , 0.        ]],\n",
       "\n",
       "       [[0.51510614, 0.26503137, 0.21986245, 0.        ],\n",
       "        [0.2503518 , 0.45823228, 0.2914159 , 0.        ]]], dtype=float32)>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_softmax(tf.random.uniform(shape = (2, 2, 4)), tf.constant([2, 3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 2, 4), dtype=float32, numpy=\n",
       "array([[[1.        , 0.        , 0.        , 0.        ],\n",
       "        [0.2759355 , 0.45967767, 0.26438686, 0.        ]],\n",
       "\n",
       "       [[0.6735375 , 0.32646254, 0.        , 0.        ],\n",
       "        [0.17813917, 0.27542904, 0.19633816, 0.3500936 ]],\n",
       "\n",
       "       [[1.        , 0.        , 0.        , 0.        ],\n",
       "        [0.5281537 , 0.4718463 , 0.        , 0.        ]]], dtype=float32)>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_softmax(tf.random.uniform((3, 2, 4)), tf.constant([[1, 3], [2, 4], [1, 2]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additive attention\n",
    "$$f(q, (k_1, v_1), (k_2, v_2), ..., (k_m, v_m)) = \\Sigma_{i = 1}^n{\\alpha{(q, k_i)}v_i}$$\n",
    "$\\mathbf{q}\\in \\mathbb{R}^q$,$\\mathbf{k}_i\\in \\mathbb{R}^k$, $\\mathbf{v}_i\\in \\mathbb{R}^v$\n",
    "\n",
    "$$\\alpha(\\mathbf{q}, \\mathbf{k}_i) = softmax(a(\\mathbf{q}, \\mathbf{k}_i)) = \\frac{exp(a(\\mathbf{q}, \\mathbf{k}_i))}\n",
    "{\\Sigma_{j=1}^nexp(a(\\mathbf{q}, \\mathbf{k}_j))}$$\n",
    "\n",
    "Attention weights = $\\alpha{(q, k_i)}$. In addtive attention,\n",
    "$$a(\\mathbf{q}, \\mathbf{k}) = \\mathbf{w}_v^Ttanh(\\mathbf{W}_q\\mathbf{q} + \\mathbf{W}_k\\mathbf{k}) \\in \\mathbb{R}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdditiveAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, key_size, query_size, num_hiddens, dropout, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.W_k = tf.keras.layers.Dense(num_hiddens, use_bias = False)\n",
    "        self.W_q = tf.keras.layers.Dense(num_hiddens, use_bias = False)\n",
    "        self.w_v = tf.keras.layers.Dense(1, use_bias = False)\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout)\n",
    "        \n",
    "    def call(self, queries, keys, values, valid_lens, **kwargs):\n",
    "        queries, keys = self.W_q(queries), self.W_k(keys)\n",
    "        # After dimension expansion, shape of `queries`: (`batch_size`, no. of\n",
    "        # queries, 1, `num_hiddens`) and shape of `keys`: (`batch_size`, 1,\n",
    "        # no. of key-value pairs, `num_hiddens`). Sum them up with\n",
    "        # broadcasting\n",
    "        features = tf.expand_dims(queries, axis = 2) + tf.expand_dims(keys, axis = 1)\n",
    "        features = tf.nn.tanh(features)\n",
    "        # There is only one output of `self.w_v`, so we remove the last\n",
    "        # one-dimensional entry from the shape. Shape of `scores`:\n",
    "        # (`batch_size`, no. of queries, no. of key-value pairs)\n",
    "        scores = tf.squeeze(self.w_v(features), axis = -1)\n",
    "        self.attention_weights = masked_softmax(scores, valid_lens)\n",
    "        # Shape of `values`: (`batch_size`, no. of key-value pairs, value\n",
    "        # dimension)\n",
    "        return tf.matmul(self.dropout(self.attention_weights, **kwargs), values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 1, 4), dtype=float32, numpy=\n",
       "array([[[ 2.,  3.,  4.,  5.]],\n",
       "\n",
       "       [[10., 11., 12., 13.]]], dtype=float32)>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queries, keys = tf.random.normal(shape = (2, 1, 20)), tf.ones((2, 10, 2))\n",
    "# The two value matrices in the `values` minibatch are identical\n",
    "values = tf.repeat(tf.reshape(tf.range(40, dtype=tf.float32), shape = (1, 10, 4)), repeats = 2, axis = 0)\n",
    "valid_lens = tf.constant([2, 6])\n",
    "\n",
    "attention = AdditiveAttention(key_size=2, query_size=20, num_hiddens=8,\n",
    "                              dropout=0.1)\n",
    "attention(queries, keys, values, valid_lens, training = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 1, 10), dtype=float32, numpy=\n",
       "array([[[0.5       , 0.5       , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ]],\n",
       "\n",
       "       [[0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667,\n",
       "         0.16666667, 0.        , 0.        , 0.        , 0.        ]]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention.attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 180x180 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_heatmaps(tf.expand_dims(tf.reshape(attention.attention_weights, shape = (1,2,10)), axis = 0),\n",
    "              xlabel='Keys',\n",
    "              ylabel='Queries')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaled dot-product attention\n",
    "$$a(\\mathbf{q}, \\mathbf{k}) = \\mathbf{q}^T\\mathbf{k}/\\sqrt{d}$$\n",
    "For $n$ queries and $m$ key-value pairs (each of dimension $d$), \n",
    "$$softmax(\\frac{\\mathbf{Q}\\mathbf{K}^T}{\\sqrt{d}})\\mathbf{V} \\in \\mathbb{R}^{n\\times v}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DotProductAttention(tf.keras.layers.Layer):\n",
    "    \"\"\"Scaled dot product attention.\"\"\"\n",
    "    def __init__(self, dropout, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout)\n",
    "        \n",
    "    # Shape of `queries`: (`batch_size`, no. of queries, `d`)\n",
    "    # Shape of `keys`: (`batch_size`, no. of key-value pairs, `d`)\n",
    "    # Shape of `values`: (`batch_size`, no. of key-value pairs, value\n",
    "    # dimension)\n",
    "    # Shape of `valid_lens`: (`batch_size`,) or (`batch_size`, no. of queries)\n",
    "    def call(self, queries, keys, values, valid_lens, **kwargs):\n",
    "        d = queries.shape[-1]\n",
    "        scores = tf.matmul(queries, keys, transpose_b = True)/tf.math.sqrt(tf.cast(d, dtype = tf.float32))\n",
    "        self.attention_weights = masked_softmax(scores, valid_lens)\n",
    "        return tf.matmul(self.dropout(self.attention_weights, **kwargs), values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 1, 4), dtype=float32, numpy=\n",
       "array([[[ 2.,  3.,  4.,  5.]],\n",
       "\n",
       "       [[10., 11., 12., 13.]]], dtype=float32)>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queries = tf.random.normal(shape = (2, 1, 2))\n",
    "attention = DotProductAttention(dropout=0.5)\n",
    "attention(queries, keys, values, valid_lens, training = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 1, 10), dtype=float32, numpy=\n",
       "array([[[0.5       , 0.5       , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ]],\n",
       "\n",
       "       [[0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667,\n",
       "         0.16666667, 0.        , 0.        , 0.        , 0.        ]]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention.attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 180x180 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_heatmaps(tf.expand_dims(tf.reshape(attention.attention_weights, shape = (1,2,10)), axis = 0),\n",
    "              xlabel='Keys',\n",
    "              ylabel='Queries')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_241_gpu",
   "language": "python",
   "name": "tf_241_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
