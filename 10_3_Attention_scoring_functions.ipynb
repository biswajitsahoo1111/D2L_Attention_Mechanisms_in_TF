{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow Version:  2.4.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Tensorflow Version: \", tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from saved_func_show_heatmaps import show_heatmaps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Masked softmax operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequence_mask(X, valid_len, value = 0):\n",
    "    \"\"\"Mask irrelevant entries in sequences.\n",
    "    Input argument X of this function is 2D.\n",
    "    \"\"\"\n",
    "    maxlen = X.shape[1]\n",
    "    mask = tf.range(start = 0, limit = maxlen, dtype = tf.float32)[None, :] < tf.cast(valid_len[:, None], dtype = tf.float32)\n",
    "    return tf.where(mask, X, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_softmax(X, valid_lens):\n",
    "    \"\"\"X: 3D tensor\n",
    "       valid_lens: 1D or 2D tensor\n",
    "       When valid_lens is 1D, its `len(valid_lens)` should be same as `X.shape[0]`.\n",
    "       When valid_lens is 2D, `valid_lens.shape` should be same as `X.shape[:2]`.\n",
    "    \"\"\"\n",
    "    if valid_lens is None:\n",
    "        return tf.nn.softmax(X, axis = -1)\n",
    "    else:\n",
    "        shape = X.shape\n",
    "        if len(valid_lens.shape) == 1:\n",
    "            if (len(valid_lens) == X.shape[0]): \n",
    "                valid_lens = tf.repeat(valid_lens, repeats = shape[1])\n",
    "            else:\n",
    "                print(\"Valid_lens shape is incompatible with input. Read docstring of `masked_softmax` function.\")\n",
    "        else:\n",
    "            if (valid_lens.shape == X.shape[:2]):\n",
    "                valid_lens = tf.reshape(valid_lens, shape = -1)\n",
    "            else:\n",
    "                print(\"Valid_lens shape is incompatible with input. Read docstring of `masked_softmax` function.\")\n",
    "            \n",
    "        \n",
    "        X = sequence_mask(tf.reshape(X, shape = (-1, shape[-1])), valid_lens, value=-1e6)    \n",
    "        return tf.nn.softmax(tf.reshape(X, shape = shape), axis = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 2, 4), dtype=float32, numpy=\n",
       "array([[[0.47316122, 0.5268388 , 0.        , 0.        ],\n",
       "        [0.50380087, 0.4961991 , 0.        , 0.        ]],\n",
       "\n",
       "       [[0.2242239 , 0.2310319 , 0.54474425, 0.        ],\n",
       "        [0.33668607, 0.3555478 , 0.3077661 , 0.        ]]], dtype=float32)>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_softmax(tf.random.uniform(shape = (2, 2, 4)), tf.constant([2, 3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 2, 4), dtype=float32, numpy=\n",
       "array([[[1.        , 0.        , 0.        , 0.        ],\n",
       "        [0.4714767 , 0.28774014, 0.24078313, 0.        ]],\n",
       "\n",
       "       [[0.624564  , 0.37543604, 0.        , 0.        ],\n",
       "        [0.26083523, 0.35128734, 0.21069005, 0.17718741]],\n",
       "\n",
       "       [[1.        , 0.        , 0.        , 0.        ],\n",
       "        [0.5287057 , 0.47129422, 0.        , 0.        ]]], dtype=float32)>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_softmax(tf.random.uniform((3, 2, 4)), tf.constant([[1, 3], [2, 4], [1, 2]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additive attention\n",
    "$$f(q, (k_1, v_1), (k_2, v_2), ..., (k_m, v_m)) = \\Sigma_{i = 1}^n{\\alpha{(q, k_i)}v_i}$$\n",
    "$\\mathbf{q}\\in \\mathbb{R}^q$,$\\mathbf{k}_i\\in \\mathbb{R}^k$, $\\mathbf{v}_i\\in \\mathbb{R}^v$\n",
    "\n",
    "$$\\alpha(\\mathbf{q}, \\mathbf{k}_i) = softmax(a(\\mathbf{q}, \\mathbf{k}_i)) = \\frac{exp(a(\\mathbf{q}, \\mathbf{k}_i))}\n",
    "{\\Sigma_{j=1}^nexp(a(\\mathbf{q}, \\mathbf{k}_j))}$$\n",
    "\n",
    "Attention weights = $\\alpha{(q, k_i)}$. In addtive attention,\n",
    "$$a(\\mathbf{q}, \\mathbf{k}) = \\mathbf{w}_v^Ttanh(\\mathbf{W}_q\\mathbf{q} + \\mathbf{W}_k\\mathbf{k}) \\in \\mathbb{R}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdditiveAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, key_size, query_size, num_hiddens, dropout, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.W_k = tf.keras.layers.Dense(num_hiddens, use_bias = False)\n",
    "        self.W_q = tf.keras.layers.Dense(num_hiddens, use_bias = False)\n",
    "        self.w_v = tf.keras.layers.Dense(1, use_bias = False)\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout)\n",
    "        \n",
    "    def call(self, queries, keys, values, valid_lens, training):\n",
    "        queries, keys = self.W_q(queries), self.W_k(keys)\n",
    "        # After dimension expansion, shape of `queries`: (`batch_size`, no. of\n",
    "        # queries, 1, `num_hiddens`) and shape of `keys`: (`batch_size`, 1,\n",
    "        # no. of key-value pairs, `num_hiddens`). Sum them up with\n",
    "        # broadcasting\n",
    "        features = tf.expand_dims(queries, axis = 2) + tf.expand_dims(keys, axis = 1)\n",
    "        features = tf.nn.tanh(features)\n",
    "        # There is only one output of `self.w_v`, so we remove the last\n",
    "        # one-dimensional entry from the shape. Shape of `scores`:\n",
    "        # (`batch_size`, no. of queries, no. of key-value pairs)\n",
    "        scores = tf.squeeze(self.w_v(features), axis = -1)\n",
    "        self.attention_weights = masked_softmax(scores, valid_lens)\n",
    "        # Shape of `values`: (`batch_size`, no. of key-value pairs, value\n",
    "        # dimension)\n",
    "        return tf.matmul(self.dropout(self.attention_weights, training = training), values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 1, 4), dtype=float32, numpy=\n",
       "array([[[ 2.,  3.,  4.,  5.]],\n",
       "\n",
       "       [[10., 11., 12., 13.]]], dtype=float32)>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queries, keys = tf.random.normal(shape = (2, 1, 20)), tf.ones((2, 10, 2))\n",
    "# The two value matrices in the `values` minibatch are identical\n",
    "values = tf.repeat(tf.reshape(tf.range(40, dtype=tf.float32), shape = (1, 10, 4)), repeats = 2, axis = 0)\n",
    "valid_lens = tf.constant([2, 6])\n",
    "\n",
    "attention = AdditiveAttention(key_size=2, query_size=20, num_hiddens=8,\n",
    "                              dropout=0.1)\n",
    "attention(queries, keys, values, valid_lens, training = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 1, 10), dtype=float32, numpy=\n",
       "array([[[0.5       , 0.5       , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ]],\n",
       "\n",
       "       [[0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667,\n",
       "         0.16666667, 0.        , 0.        , 0.        , 0.        ]]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention.attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 180x180 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_heatmaps(tf.expand_dims(tf.reshape(attention.attention_weights, shape = (1,2,10)), axis = 0),\n",
    "              xlabel='Keys',\n",
    "              ylabel='Queries')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaled dot-product attention\n",
    "$$a(\\mathbf{q}, \\mathbf{k}) = \\mathbf{q}^T\\mathbf{k}/\\sqrt{d}$$\n",
    "For $n$ queries and $m$ key-value pairs (each of dimension $d$), \n",
    "$$softmax(\\frac{\\mathbf{Q}\\mathbf{K}^T}{\\sqrt{d}})\\mathbf{V} \\in \\mathbb{R}^{n\\times v}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DotProductAttention(tf.keras.layers.Layer):\n",
    "    \"\"\"Scaled dot product attention.\"\"\"\n",
    "    def __init__(self, dropout, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout)\n",
    "        \n",
    "    # Shape of `queries`: (`batch_size`, no. of queries, `d`)\n",
    "    # Shape of `keys`: (`batch_size`, no. of key-value pairs, `d`)\n",
    "    # Shape of `values`: (`batch_size`, no. of key-value pairs, value\n",
    "    # dimension)\n",
    "    # Shape of `valid_lens`: (`batch_size`,) or (`batch_size`, no. of queries)\n",
    "    def call(self, queries, keys, values, valid_lens, training):\n",
    "        d = queries.shape[-1]\n",
    "        scores = tf.matmul(queries, keys, transpose_b = True)/tf.math.sqrt(tf.cast(d, dtype = tf.float32))\n",
    "        self.attention_weights = masked_softmax(scores, valid_lens)\n",
    "        return tf.matmul(self.dropout(self.attention_weights, training = training), values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 1, 4), dtype=float32, numpy=\n",
       "array([[[ 2.,  3.,  4.,  5.]],\n",
       "\n",
       "       [[10., 11., 12., 13.]]], dtype=float32)>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queries = tf.random.normal(shape = (2, 1, 2))\n",
    "attention = DotProductAttention(dropout=0.5)\n",
    "attention(queries, keys, values, valid_lens, training = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 1, 10), dtype=float32, numpy=\n",
       "array([[[0.5       , 0.5       , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ]],\n",
       "\n",
       "       [[0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667,\n",
       "         0.16666667, 0.        , 0.        , 0.        , 0.        ]]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention.attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 180x180 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_heatmaps(tf.expand_dims(tf.reshape(attention.attention_weights, shape = (1,2,10)), axis = 0),\n",
    "              xlabel='Keys',\n",
    "              ylabel='Queries')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_24_env",
   "language": "python",
   "name": "tf_24_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
