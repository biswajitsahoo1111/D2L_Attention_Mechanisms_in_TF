{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow Version:  2.5.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Tensorflow Version: \", tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from saved_func_show_heatmaps import show_heatmaps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Masked softmax operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequence_mask(X, valid_len, value = 0):\n",
    "    \"\"\"Mask irrelevant entries in sequences.\n",
    "    Argument:\n",
    "        X: either a 2D or 3D tensor\n",
    "        valid_len: 1D tensor\n",
    "        value: value to be substitued for mask\n",
    "    \"\"\"\n",
    "    maxlen = X.shape[1]\n",
    "    mask = tf.range(start = 0, limit = maxlen, dtype = tf.float32)[None, :] < tf.cast(valid_len[:, None], dtype = tf.float32)\n",
    "    \n",
    "    if len(X.shape) == 3:\n",
    "        return tf.where(tf.expand_dims(mask, axis = -1), X, value)\n",
    "    else:\n",
    "        return tf.where(mask, X, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_softmax(X, valid_lens):\n",
    "    \"\"\"X: 3D tensor\n",
    "       valid_lens: 1D or 2D tensor\n",
    "       When valid_lens is 1D, its `len(valid_lens)` should be same as `X.shape[0]`.\n",
    "       When valid_lens is 2D, `valid_lens.shape` should be same as `X.shape[:2]`.\n",
    "    \"\"\"\n",
    "    if valid_lens is None:\n",
    "        return tf.nn.softmax(X, axis = -1)\n",
    "    else:\n",
    "        shape = X.shape\n",
    "        if len(valid_lens.shape) == 1:\n",
    "            if (len(valid_lens) == X.shape[0]): \n",
    "                valid_lens = tf.repeat(valid_lens, repeats = shape[1])\n",
    "            else:\n",
    "                print(\"Valid_lens shape is incompatible with input. Read docstring of `masked_softmax` function.\")\n",
    "        else:\n",
    "            if (valid_lens.shape == X.shape[:2]):\n",
    "                valid_lens = tf.reshape(valid_lens, shape = -1)\n",
    "            else:\n",
    "                print(\"Valid_lens shape is incompatible with input. Read docstring of `masked_softmax` function.\")\n",
    "            \n",
    "        \n",
    "        X = sequence_mask(tf.reshape(X, shape = (-1, shape[-1])), valid_lens, value=-1e6)    \n",
    "        return tf.nn.softmax(tf.reshape(X, shape = shape), axis = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 2, 4), dtype=float32, numpy=\n",
       "array([[[0.4410414 , 0.5589586 , 0.        , 0.        ],\n",
       "        [0.55399334, 0.44600666, 0.        , 0.        ]],\n",
       "\n",
       "       [[0.26242447, 0.20841406, 0.52916145, 0.        ],\n",
       "        [0.41934618, 0.3775574 , 0.20309637, 0.        ]]], dtype=float32)>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_softmax(tf.random.uniform(shape = (2, 2, 4)), tf.constant([2, 3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 2, 4), dtype=float32, numpy=\n",
       "array([[[1.        , 0.        , 0.        , 0.        ],\n",
       "        [0.38514763, 0.39772394, 0.21712847, 0.        ]],\n",
       "\n",
       "       [[0.49383965, 0.5061603 , 0.        , 0.        ],\n",
       "        [0.24477276, 0.17789136, 0.1766553 , 0.40068054]],\n",
       "\n",
       "       [[1.        , 0.        , 0.        , 0.        ],\n",
       "        [0.3491274 , 0.65087265, 0.        , 0.        ]]], dtype=float32)>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_softmax(tf.random.uniform((3, 2, 4)), tf.constant([[1, 3], [2, 4], [1, 2]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additive attention\n",
    "$$f(q, (k_1, v_1), (k_2, v_2), ..., (k_m, v_m)) = \\Sigma_{i = 1}^n{\\alpha{(q, k_i)}v_i}$$\n",
    "$\\mathbf{q}\\in \\mathbb{R}^q$,$\\mathbf{k}_i\\in \\mathbb{R}^k$, $\\mathbf{v}_i\\in \\mathbb{R}^v$\n",
    "\n",
    "$$\\alpha(\\mathbf{q}, \\mathbf{k}_i) = softmax(a(\\mathbf{q}, \\mathbf{k}_i)) = \\frac{exp(a(\\mathbf{q}, \\mathbf{k}_i))}\n",
    "{\\Sigma_{j=1}^nexp(a(\\mathbf{q}, \\mathbf{k}_j))}$$\n",
    "\n",
    "Attention weights = $\\alpha{(q, k_i)}$. In addtive attention,\n",
    "$$a(\\mathbf{q}, \\mathbf{k}) = \\mathbf{w}_v^Ttanh(\\mathbf{W}_q\\mathbf{q} + \\mathbf{W}_k\\mathbf{k}) \\in \\mathbb{R}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdditiveAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, key_size, query_size, num_hiddens, dropout, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.W_k = tf.keras.layers.Dense(num_hiddens, use_bias = False)\n",
    "        self.W_q = tf.keras.layers.Dense(num_hiddens, use_bias = False)\n",
    "        self.w_v = tf.keras.layers.Dense(1, use_bias = False)\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout)\n",
    "        \n",
    "    def call(self, queries, keys, values, valid_lens, **kwargs):\n",
    "        queries, keys = self.W_q(queries), self.W_k(keys)\n",
    "        # After dimension expansion, shape of `queries`: (`batch_size`, no. of\n",
    "        # queries, 1, `num_hiddens`) and shape of `keys`: (`batch_size`, 1,\n",
    "        # no. of key-value pairs, `num_hiddens`). Sum them up with\n",
    "        # broadcasting\n",
    "        features = tf.expand_dims(queries, axis = 2) + tf.expand_dims(keys, axis = 1)\n",
    "        features = tf.nn.tanh(features)\n",
    "        # There is only one output of `self.w_v`, so we remove the last\n",
    "        # one-dimensional entry from the shape. Shape of `scores`:\n",
    "        # (`batch_size`, no. of queries, no. of key-value pairs)\n",
    "        scores = tf.squeeze(self.w_v(features), axis = -1)\n",
    "        self.attention_weights = masked_softmax(scores, valid_lens)\n",
    "        # Shape of `values`: (`batch_size`, no. of key-value pairs, value\n",
    "        # dimension)\n",
    "        return tf.matmul(self.dropout(self.attention_weights, **kwargs), values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 1, 4), dtype=float32, numpy=\n",
       "array([[[ 2.,  3.,  4.,  5.]],\n",
       "\n",
       "       [[10., 11., 12., 13.]]], dtype=float32)>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queries, keys = tf.random.normal(shape = (2, 1, 20)), tf.ones((2, 10, 2))\n",
    "# The two value matrices in the `values` minibatch are identical\n",
    "values = tf.repeat(tf.reshape(tf.range(40, dtype=tf.float32), shape = (1, 10, 4)), repeats = 2, axis = 0)\n",
    "valid_lens = tf.constant([2, 6])\n",
    "\n",
    "attention = AdditiveAttention(key_size=2, query_size=20, num_hiddens=8,\n",
    "                              dropout=0.1)\n",
    "attention(queries, keys, values, valid_lens, training = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 1, 10), dtype=float32, numpy=\n",
       "array([[[0.5       , 0.5       , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ]],\n",
       "\n",
       "       [[0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667,\n",
       "         0.16666667, 0.        , 0.        , 0.        , 0.        ]]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention.attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 180x180 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_heatmaps(tf.expand_dims(tf.reshape(attention.attention_weights, shape = (1,2,10)), axis = 0),\n",
    "              xlabel='Keys',\n",
    "              ylabel='Queries')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaled dot-product attention\n",
    "$$a(\\mathbf{q}, \\mathbf{k}) = \\mathbf{q}^T\\mathbf{k}/\\sqrt{d}$$\n",
    "For $n$ queries and $m$ key-value pairs (each of dimension $d$), \n",
    "$$softmax(\\frac{\\mathbf{Q}\\mathbf{K}^T}{\\sqrt{d}})\\mathbf{V} \\in \\mathbb{R}^{n\\times v}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DotProductAttention(tf.keras.layers.Layer):\n",
    "    \"\"\"Scaled dot product attention.\"\"\"\n",
    "    def __init__(self, dropout, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout)\n",
    "        \n",
    "    # Shape of `queries`: (`batch_size`, no. of queries, `d`)\n",
    "    # Shape of `keys`: (`batch_size`, no. of key-value pairs, `d`)\n",
    "    # Shape of `values`: (`batch_size`, no. of key-value pairs, value\n",
    "    # dimension)\n",
    "    # Shape of `valid_lens`: (`batch_size`,) or (`batch_size`, no. of queries)\n",
    "    def call(self, queries, keys, values, valid_lens, **kwargs):\n",
    "        d = queries.shape[-1]\n",
    "        scores = tf.matmul(queries, keys, transpose_b = True)/tf.math.sqrt(tf.cast(d, dtype = tf.float32))\n",
    "        self.attention_weights = masked_softmax(scores, valid_lens)\n",
    "        return tf.matmul(self.dropout(self.attention_weights, **kwargs), values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 1, 4), dtype=float32, numpy=\n",
       "array([[[ 2.,  3.,  4.,  5.]],\n",
       "\n",
       "       [[10., 11., 12., 13.]]], dtype=float32)>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queries = tf.random.normal(shape = (2, 1, 2))\n",
    "attention = DotProductAttention(dropout=0.5)\n",
    "attention(queries, keys, values, valid_lens, training = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 1, 10), dtype=float32, numpy=\n",
       "array([[[0.5       , 0.5       , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ]],\n",
       "\n",
       "       [[0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667,\n",
       "         0.16666667, 0.        , 0.        , 0.        , 0.        ]]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention.attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 180x180 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_heatmaps(tf.expand_dims(tf.reshape(attention.attention_weights, shape = (1,2,10)), axis = 0),\n",
    "              xlabel='Keys',\n",
    "              ylabel='Queries')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_250_env",
   "language": "python",
   "name": "tf_250_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
